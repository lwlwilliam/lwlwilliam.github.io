---
title: 部署私有 LLM
layout: post
categories: [AI]
keywords: AI,LLM,Ollama,Anything LLM
---

目前对`LLM(Large Language Models，大语言模型)`一无所知，万事开头难，虽然部署了，但并不知道原理，不过起码兴趣点上了，好歹以兴趣入手，搞一个私有的知识库倒是不错。

还是得`Linux`环境方便，`Ollama`和`Anything LLM`一起构成了我所要的知识库。`Ollama`应该是目前本地构建运行`LLM`最好的工具了，但我不仅是想本地构建，而且要是私人知识库，因此，嵌入自己的知识数据就是另一个重要的部分。以我目前粗浅的见识来看，`Ollama`给`LLM`嵌入数据还是需要其它工具协助的，`Anything LLM`正好符合我的要求。

`Anything LLM`有很好的易用性，而且还有开箱即用的`RAG(检索增强生成，Retrieval-augmented Generation)`、`AI Agents(人工智能体)`等工具。`RAG`生成模型结合了语言模型和信息检索技术，当模型需要生成信息时，它会从一个庞大的文档集合中检索相关数据，然后利用其生成对应的信息，从而提高预测的质量和准确性；`AI Agents`与传统的人工智能相比，可以通过调用工具逐步完成既定目标。

`Ollama`加`Anything LLM`的优点这么多，部署起来却一点也不难，前提是会使用`Docker`。以下是部署命令：

```bash
$ docker network create llm
$ docker run -d -ti --name ollama --network llm ollama/ollama
$ export STORAGE_LOCATION=$HOME/anythingllm && \
mkdir -p $STORAGE_LOCATION && \
chmod 777 $STORAGE_LOCATION && \
touch "$STORAGE_LOCATION/.env" && \
chmod 777 "$STORAGE_LOCATION/.env" && \
docker run -d -p 3001:3001 \
-e OPENAI_API_KEY=foo \
--network llm \
--name anythingllm \
--cap-add SYS_ADMIN \
-v ${STORAGE_LOCATION}:/app/server/storage \
-v ${STORAGE_LOCATION}/.env:/app/server/.env \
-e STORAGE_DIR="/app/server/storage" \
mintplexlabs/anythingllm:master
```

首先创建一个网络`llm`，让`Ollama`和`Anything LLM`们于同一网络，毕竟它们之间需要通信。`Ollama`的安装倒没什么可说的，`Anything LLM`的安装命令根据实际情况对官网的进行了一定修改。其中包含两个`chmod`命令，如果不加权限，可能由于`Docker`跟宿主系统用户不一致导致权限不足；另外`-e OPENAI_API_KEY=foo`这个是因为`Anything LLM`也支持选择`openai`，需要为它添加一个环境变量`OPENAI_API_KEY`，至于变量值如果不使用的话填一个非空值即可，这两个问题可能是导致`Anything LLM`运行异常的重要原因。

还有一个值得注意的是在`Anything LLM`界面选择模型运行工具时，选择`Ollama`填入的链接上填入`http://ollama:11434`，这个“域名”`ollama`就是`Docker`容器的名称。

至此，已经将本地的`LLM`部署完成，通过`http://localhost:3001`即可访问，通过上述的步骤即可看到以下界面。

![document](/assets/images/2024/0508/20240508-161906.png)

上图红色方框的按钮用于上传个人文件，“训练”大模型，让其成为知识库，支持多种格式文件。除了界面外，`Anything LLM`还开放了`API`，便于企业对接嵌入内部数据，形成企业领域专用模型。

服务器性能不要太差喔。